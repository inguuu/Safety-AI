from openai import AzureOpenAI
from dotenv import load_dotenv
import os
import streamlit as st;

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError, ClientAuthenticationError
import sys

# .env file ì°¸ì¡°
load_dotenv()

AZURE_SEARCH_ENDPOINT = os.getenv('AZURE_SEARCH_ENDPOINT')
AZURE_SEARCH_API_KEY = os.getenv('AZURE_SEARCH_API_KEY')
AZURE_DEPLOYMENT_MODEL = "gpt-4.1-mini"  
AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')
AZURE_OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

try:
    search_credential = AzureKeyCredential(AZURE_SEARCH_API_KEY)

    openai_client = AzureOpenAI(
        api_version="2024-06-01",
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        api_key=AZURE_OPENAI_API_KEY
    )

    search_client = SearchClient(
        endpoint=AZURE_SEARCH_ENDPOINT,
        index_name="risk-assessment-index",
        credential=search_credential
    )
    
except ClientAuthenticationError as e:
    print("API KEYë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”")
    sys.exit(1)
except HttpResponseError as e:
    print("ì—”ë“œí¬ì¸íŠ¸ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”")
    sys.exit(1)
except Exception as e:
    print("ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    print(e)
    sys.exit(1)

# ê±´ê°•ê²€ì§„ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ë¡œ ë³€ê²½
GROUNDED_PROMPT = """
You are a friendly and knowledgeable health screening assistant.
Answer the user's query strictly based on the information from the provided sources below.
Use a clear, concise, and friendly style, in bullet points if appropriate.
Do NOT invent any information beyond the sources.
If the information is insufficient, say you don't know.
Query: {query}
Sources:
{sources}
"""


SYNONYM_MAP = {
    "ì§„ë£Œí•­ëª©": "ê²€ì‚¬í•­ëª©",
    "ê²€ì§„í•­ëª©": "ê²€ì‚¬í•­ëª©",
    "ê¸°ê´€": "ì„¼í„°",
    # í•„ìš”ì‹œ ë” ì¶”ê°€
}


def generate_synonym_prompt(raw_query: str, synonym_map: dict) -> str:
    mapping_lines = "\n".join(
        [f"'{k}'ëŠ” '{v}'ë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”." for k, v in synonym_map.items()]
    )
    
    prompt = f"""
    ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì•„ë˜ ë‹¨ì–´ë“¤ì„ ê°ê° ëŒ€ì‘í•˜ëŠ” ë‹¨ì–´ë¡œ ë°”ê¾¸ì–´ ì£¼ì„¸ìš”.
    ë‹¨ì–´ ë§¤í•‘:
    {mapping_lines}
    ë¬¸ì¥: "{raw_query}"
    ë¬¸ë§¥ì´ ìì—°ìŠ¤ëŸ¬ìš´ í˜•íƒœë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.
    """
    return prompt


def preprocess_query_with_openai(raw_query: str, synonym_map: dict) -> str:
    prompt = generate_synonym_prompt(raw_query, synonym_map)
    
    response = openai_client.chat.completions.create(
        model=AZURE_DEPLOYMENT_MODEL,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=100,
        temperature=0,
    )
    processed_query = response.choices[0].message.content.strip()
    return processed_query


# LLM ì‘ë‹µ í•¨ìˆ˜ (AISEARCH + GPT í˜¸ì¶œ í¬í•¨)
def get_grounded_response(messages: list):
    user_query = messages[-1]['content']  # ë§ˆì§€ë§‰ ìœ ì € ë©”ì‹œì§€ ê¸°ì¤€

    # 1. ì „ì²˜ë¦¬í•´ì„œ ìœ ì‚¬ì–´ ì¹˜í™˜
    processed_query = preprocess_query_with_openai(user_query, SYNONYM_MAP)


    # 2. ì›ë³¸ ì¿¼ë¦¬ì™€ ì¹˜í™˜ëœ ì¿¼ë¦¬ë¥¼ ëª¨ë‘ í¬í•¨í•´ì„œ ê²€ìƒ‰ì–´ êµ¬ì„±
    combined_query = f'"{user_query}" OR "{processed_query}"'
    try:
        # Azure Searchì—ì„œ ê²€ìƒ‰
        search_result = search_client.search(
            search_text=combined_query,
            top=5,
            select="Content,DocumentName,Tags",
        )
        search_results_list = list(search_result)
    except Exception as e:
        return f"ê²€ìƒ‰ ì˜¤ë¥˜ ë°œìƒ: {e}"

    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ LLM í˜¸ì¶œ
    if not search_results_list:
        response = openai_client.chat.completions.create(
            model=AZURE_DEPLOYMENT_MODEL,
            messages=messages,
            max_tokens=800,
            temperature=0.7,
        )
        llm_answer = response.choices[0].message.content
        if not st.session_state.shown_no_search_msg:
            st.session_state.shown_no_search_msg = True
            return "ì›í•˜ì‹œëŠ” ë‹µë³€ì„ ì°¾ì§€ ëª»í•´, ì œê°€ ëŒ€ë‹µí•´ë“œë¦½ë‹ˆë‹¤: " + llm_answer
        else:
            return llm_answer

    sources_formatted = "\n".join([
        f'{doc["DocumentName"]}: {doc["Content"]}: {doc.get("Tags", [])}'
        for doc in search_results_list
    ])

    # GPT ì…ë ¥ ë©”ì‹œì§€
    messages = [
        {"role": "user",
         "content": GROUNDED_PROMPT.format(query=processed_query, sources=sources_formatted)}
    ]

    # GPT í˜¸ì¶œ
    response = openai_client.chat.completions.create(
        model=AZURE_DEPLOYMENT_MODEL,
        messages=messages,
        max_tokens=800,
        temperature=0.7,
    )

    return response.choices[0].message.content


# ------------------------------------------
# ğŸ¨ í”„ë¡ íŠ¸ì—”ë“œ: Streamlit UI (ChatGPT ìŠ¤íƒ€ì¼)
# ------------------------------------------
st.set_page_config(page_title="ê±´ê°•ê²€ì§„ ìœ„í—˜ì„±í‰ê°€ Agent", page_icon="ğŸ©º", layout="centered")

# ğŸ’… ìŠ¤íƒ€ì¼: ChatGPT í™ˆí˜ì´ì§€ ìœ ì‚¬
st.markdown("""
    <style>
        html, body {
            background-color: #ffffff;
            font-family: 'Segoe UI', sans-serif;
        }

        .user-message, .assistant-message {
            padding: 16px;
            border-radius: 12px;
            margin: 16px auto;
            max-width: 700px;
            font-size: 16px;
            line-height: 1.6;
        }

        .user-message {
            background-color: #f1f1f1;
            color: #000;
        }

        .assistant-message {
            background-color: #e8f0fe;
            color: #000;
        }

        .stChatInput input {
            padding: 12px;
            font-size: 16px;
            border-radius: 12px !important;
            border: 1px solid #ccc;
        }

        h1 {
            text-align: center;
            font-weight: 600;
            margin-top: 40px;
            margin-bottom: 20px;
        }

        .stChatMessage {
            display: flex;
            justify-content: center;
        }

        footer, header {visibility: hidden;}
    </style>
""", unsafe_allow_html=True)
# -----------------------
# ğŸ¨ Streamlit UI ë””ìì¸
# -----------------------
st.title("ğŸ©º ê±´ê°•ê²€ì§„ ìœ„í—˜ì„±í‰ê°€ Agent")
st.write("<div style='text-align:center;'>ê±´ê°•ê²€ì§„ ê´€ë ¨ ìœ„í—˜ìš”ì¸ì„ ì§ˆë¬¸í•´ë³´ì„¸ìš” ğŸ¤–</div>", unsafe_allow_html=True)

# 1) ì§ˆë¬¸ ì˜ˆì‹œ íŒíŠ¸ ë°•ìŠ¤ ì¶”ê°€
st.info(
    """
    ğŸ’¡ **ì§ˆë¬¸ ì˜ˆì‹œ:**
    - ì•¼ê°„ ê·¼ë¬´ê°€ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?
    - ê³ í˜ˆì•• ìœ„í—˜ìš”ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?
    - ê±´ê°•ê²€ì§„ì—ì„œ ê¼­ í™•ì¸í•´ì•¼ í•˜ëŠ” í•­ëª©ì€?
    """
)

# 2) ì§ˆë¬¸ ê°€ì´ë“œ ë„ì›€ë§ í† ê¸€(expander) ì¶”ê°€
with st.expander("â“ ì§ˆë¬¸ ê°€ì´ë“œ ë³´ê¸°"):
    st.write(
        """
        - **ì§„ë£Œí•­ëª©**, **ê²€ì§„í•­ëª©** ëŒ€ì‹  **ê²€ì‚¬í•­ëª©**ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.
        - ê¸°ê´€ ëŒ€ì‹  **ì„¼í„°**ë¼ëŠ” ë‹¨ì–´ë„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
        - ì˜ˆë¥¼ ë“¤ì–´, â€œê²€ì§„ì„¼í„°ì˜ í˜ˆì•¡ê²€ì‚¬ í•­ëª©ì€ ë¬´ì—‡ì¸ê°€ìš”?â€ ì²˜ëŸ¼ ì§ˆë¬¸í•´ë³´ì„¸ìš”.
        """
    )


# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'shown_no_search_msg' not in st.session_state:
    st.session_state.shown_no_search_msg = False

# ì´ì „ ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(
            f"<div class='{msg['role']}-message'>{msg['content']}</div>",
            unsafe_allow_html=True
        )

user_input_placeholder = (
    "ì˜ˆ) ì•¼ê°„ ê·¼ë¬´ê°€ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?\n"
    "ì˜ˆ) ê³ í˜ˆì•• ìœ„í—˜ìš”ì¸ì„ ì•Œë ¤ì¤˜ìš”\n"
    "ì˜ˆ) ê±´ê°•ê²€ì§„ ê²€ì‚¬í•­ëª©ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜ìš”"
)

# ì‚¬ìš©ì ì…ë ¥
if user_input := st.chat_input("ì˜ˆ: ì•¼ê°„ ê·¼ë¬´ê°€ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(f"<div class='user-message'>{user_input}</div>", unsafe_allow_html=True)

    with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        assistant_response = get_grounded_response(st.session_state.messages)

    st.session_state.messages.append({"role": "assistant", "content": assistant_response})
    with st.chat_message("assistant"):
        st.markdown(f"<div class='assistant-message'>{assistant_response}</div>", unsafe_allow_html=True)